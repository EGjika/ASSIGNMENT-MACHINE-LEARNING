{
  "metadata": {
    "kernelspec": {
      "display_name": "ml38",
      "language": "python",
      "name": "ml38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# ASSIGNMENT 1: CARS DATASET\n\n## Eralda Gjika\n\nDataset\n The Cars Dataset can be found here: (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) \n The Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. \nObjective \n Your clients would like a model that can classify cars found from images taken from traffic cameras. \n  This assignment will involve exploring the importance of the size of a labelled dataset for supervised learning. \nNote: For questions where you are asked to explain or interpret results, please do so within the Jupyter \nnotebook (or equivalent) using Markdown \n",
      "metadata": {},
      "id": "fbacf982-b458-417e-bb3f-d7db0c45b1ff"
    },
    {
      "cell_type": "markdown",
      "source": "# Proposed solution",
      "metadata": {
        "tags": [],
        "jp-MarkdownHeadingCollapsed": true
      },
      "id": "8e8d7877"
    },
    {
      "cell_type": "markdown",
      "source": "# Import Dependencies",
      "metadata": {
        "tags": []
      },
      "id": "d5f2da60-4e37-49cc-a41b-f79e481c1372"
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nfrom typing import List\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet18\nfrom torchvision import transforms as T\nfrom tqdm import tqdm\nimport gzip\nimport tarfile\nfrom scipy.io import loadmat\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nimport matplotlib.pyplot as plt",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "c98baf2c"
    },
    {
      "cell_type": "markdown",
      "source": "## Set seeds for reproducibility",
      "metadata": {},
      "id": "b7e000c6"
    },
    {
      "cell_type": "code",
      "source": "SEED = 12345\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e681c79a"
    },
    {
      "cell_type": "markdown",
      "source": "## Extract the data from .tar files",
      "metadata": {},
      "id": "aa4129c2"
    },
    {
      "cell_type": "code",
      "source": "my_tar = tarfile.open('./cars_train.tar')\nmy_tar.extractall('.') # specify which folder to extract to\nmy_tar.close()\n\n# my_tar = tarfile.open('./car_devkit.tar')\n# my_tar.extractall('.') # specify which folder to extract to\n# my_tar.close()\n\nmy_tar = tarfile.open('./cars_test.tar')\nmy_tar.extractall('.') # specify which folder to extract to\nmy_tar.close()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "77cf2bba"
    },
    {
      "cell_type": "code",
      "source": "train_annots = loadmat(\"./devkit/cars_train_annos.mat\")['annotations']\ntrain_classes = train_annots['class'][0]\ntrain_fnames = train_annots['fname'][0]\ntrain_label_dict = {filename.item(): cls.item() for filename, cls in zip(train_fnames, train_classes)}\n\n# test_annots = loadmat(\"./devkit/cars_test_annos.mat\")['annotations']\n# test_fnames = test_annots['fname'][0]\n# test_label_dict = [filename.item(): cls.item() for filename, cls in test_fnames]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ac4a0de7"
    },
    {
      "cell_type": "code",
      "source": "def remove_proportion_of_labels_for_each_class(dataset_labels: List[int],  proportion: float) -> None:\n    n = len(dataset_labels)\n    dataset_lbls_arr = np.array(dataset_labels)\n    all_idxs = np.arange(n)\n    for lbl in set(dataset_labels):\n        # get all indices where we have the same class label: lbl \n        lbl_idxs = all_idxs[np.array(dataset_labels) == lbl]\n        # randomly choose int(proportion*len(lbl_idxs) labels to be removed \n        idxs_to_be_dropped = list(np.random.choice(lbl_idxs, int(proportion*len(lbl_idxs)), replace=False))\n    # set the index to -1 (-1 representing unlabelled data)\n    for idx in idxs_to_be_dropped:\n        dataset_lbls_arr[idx] = -1\n    return dataset_labels, dataset_lbls_arr",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4f088c98"
    },
    {
      "cell_type": "markdown",
      "source": "### Task 1:  Build a function that converts a labelled dataset into labelled and unlabelled subsets.",
      "metadata": {},
      "id": "bc483a56"
    },
    {
      "cell_type": "code",
      "source": "def remove_proportion_of_labels_for_each_class(dataset_labels,  proportion):\n    n = len(dataset_labels)\n    dataset_lbls_arr = np.array(dataset_labels)\n    all_idxs = np.arange(n)\n    while True:\n        again = False\n        idxs_to_be_dropped = list(np.random.choice(all_idxs, int(proportion*n), replace=False))\n        for lbl in set(dataset_labels):\n            count = 0\n            for idx in idxs_to_be_dropped:\n                if dataset_labels[idx] == lbl:\n                    count+=1 \n            if count == len(dataset_lbls_arr[dataset_lbls_arr == lbl]):\n                again = True\n                break\n        if not again:\n            break\n    for idx in idxs_to_be_dropped:\n        dataset_lbls_arr[idx] = -1\n    return dataset_labels, dataset_lbls_arr\n        ",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "008f9f65"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 2 : Data cleaning",
      "metadata": {},
      "id": "49ef7b08"
    },
    {
      "cell_type": "code",
      "source": "img_to_be_deleted = []\nfor filename in train_label_dict:\n    # load the image\n    img = Image.open(f\"./cars_train/{filename}\")\n    # check if it's not RGB\n    if img.getbands() != ('R','G','B'):\n        # append it to the list\n        img_to_be_deleted.append(filename)\n\nfor filename in img_to_be_deleted:\n    # remove the filename from the cars_train folder\n    os.remove(f\"./cars_train/{filename}\")\n    # remove \n    del train_label_dict[filename]\n        ",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9c631c14"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 3 :Dataset representation",
      "metadata": {},
      "id": "9270ab0d"
    },
    {
      "cell_type": "markdown",
      "source": "#### Load ResNet18 from PyTorch",
      "metadata": {},
      "id": "eb272f29"
    },
    {
      "cell_type": "code",
      "source": "model = resnet18(pretrained=True)\n# replace the last resnet18 layer with Identity layer\nmodel.fc = nn.Identity() # now, model output will be a vector of size 512",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "76f7d41f"
    },
    {
      "cell_type": "markdown",
      "source": "#### Iterate over each image to get the embedding",
      "metadata": {},
      "id": "06e18faa"
    },
    {
      "cell_type": "code",
      "source": "full_dataset = {}\n\nimg_transform = T.Compose([\n                             T.Resize((224, 224)),\n                             T.ToTensor(),\n                             T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])\n\n\nfor i, (k,v) in tqdm(enumerate(train_label_dict.items())):\n    img = img_transform(Image.open(f\"./cars_train/{k}\")).unsqueeze(dim=0) # batch = 1\n    embedding = model(img)\n    full_dataset[i+1] = {\"embedding\": embedding.detach().numpy(), \n                         \"class_idx\": v,\n                         \"labelled\": 1,\n                        } ",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ee750c81"
    },
    {
      "cell_type": "code",
      "source": "torch.save(full_dataset, \"./full_dataset.pt\")",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "e1400d2f"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 4: Build a partially labelled dataset",
      "metadata": {},
      "id": "fa7ceb9c"
    },
    {
      "cell_type": "code",
      "source": "full_dataset = torch.load(\"./full_dataset.pt\")",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4d1a0382"
    },
    {
      "cell_type": "code",
      "source": "original_labels, lbls_to_remove = remove_proportion_of_labels_for_each_class(\n    [v['class_idx'] for _, v in full_dataset.items()], \n    0.6,\n)\npartial_dataset = full_dataset.copy()\nfor i,(k, v) in enumerate(partial_dataset.items()):\n    if lbls_to_remove[i] == -1:\n        v['labelled'] = 0",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9a746578"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 5: Create train/validation split",
      "metadata": {},
      "id": "4b18e68a"
    },
    {
      "cell_type": "code",
      "source": "def split_dataset(dataset_inputs, dataset_labels, training_proportion):\n    training_inputs, test_inputs, training_labels, test_labels = train_test_split(dataset_inputs, \n                                                                                  dataset_labels, \n                                                                                  train_size=training_proportion\n                                                                                 )\n    return np.array(training_inputs), np.array(training_labels), np.array(test_inputs), np.array(test_labels)",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "9691717c"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 6: Create experiment(s) to convince clients that more labelled data will improve model performance \n",
      "metadata": {},
      "id": "55622c4a"
    },
    {
      "cell_type": "code",
      "source": "unlabelled_accuracies = []\ndataset_inputs = [v['embedding'] for _, v in partial_dataset.items() if v['labelled']==1]\n# subtract 1 since the labels start from 1\ndataset_labels = [v['class_idx'] - 1 for _, v in partial_dataset.items() if v['labelled']==1]\n\nunlabelled_inputs = [v['embedding'] for _, v in partial_dataset.items() if v['labelled']==0]\n# subtract 1 since the labels start from 1\noriginal_labels_for_unlabelled_data = [v['class_idx']-1 for _, v in partial_dataset.items() if v['labelled']==0]\n\nfor training_proportion in tqdm([0.5, 0.7, 0.9]):\n    training_inputs, training_labels, _, _ = split_dataset(\n        dataset_inputs,\n        dataset_labels,\n        training_proportion\n    )\n    classifier = SGDClassifier(loss='modified_huber', max_iter=1000, random_state=54321)\n    classifier.fit(training_inputs[:,0,:], training_labels)\n    \n    preds = classifier.predict(np.array(unlabelled_inputs)[:,0,:])\n    unlabelled_accuracies.append(np.mean(preds == np.array(original_labels_for_unlabelled_data)))\n",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "46e21b52"
    },
    {
      "cell_type": "code",
      "source": "l = len(dataset_inputs)\nplt.plot([int(0.5*l), int(0.7*l), int(0.9*l)], \n         [acc*100 for acc in unlabelled_accuracies], \"b+--\")\nplt.xlabel(\"Number of labelled data\")\nplt.ylabel(\"Accuracy on unlabelled data (%)\")\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "eb03add9"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 7:Active learning to select new instances to be labelled",
      "metadata": {},
      "id": "e8f43b43"
    },
    {
      "cell_type": "code",
      "source": "classifier = SGDClassifier(loss='modified_huber', max_iter=5000, random_state=12345)\nclassifier.fit(np.array(dataset_inputs)[:,0,:], np.array(dataset_labels))\n\nprobs = classifier.predict_proba(np.array(unlabelled_inputs)[:,0,:])",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "4f7bba4d"
    },
    {
      "cell_type": "code",
      "source": "from scipy.stats import entropy\n\none_hot_labels = np.eye(196)[original_labels_for_unlabelled_data]\nentropies = [entropy(one_hot_labels[i], probs[i]) for i in range(probs.shape[0])]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "8082ce91"
    },
    {
      "cell_type": "code",
      "source": "K = int(25* 100/60 * len(entropies))\nindices_to_be_appended = np.argsort(entropies)[::-1][:K]",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "63722268"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "ae56369d"
    },
    {
      "cell_type": "markdown",
      "source": "## Task 8:Final model training and evaluation",
      "metadata": {
        "tags": []
      },
      "id": "464ad7aa-051f-463c-92f2-15e73f097729"
    },
    {
      "cell_type": "markdown",
      "source": "This final part is missing. The challenge was a great one with a lot of ideas on what libraries and functions to use. Combination of many statistical approaches and applications. \nThank you a lot!\nIt was fun and made me learn more!\n\n02 December 2022\n\nGithub account: https://github.com/EGjika ",
      "metadata": {
        "tags": []
      },
      "id": "e46da925-8c20-4060-ac9b-63470ba4911b"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "abecf6af-0584-4f9e-a30a-74a7bdfd1654"
    }
  ]
}